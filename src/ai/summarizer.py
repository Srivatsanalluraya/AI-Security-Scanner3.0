#!/usr/bin/env python3
"""
Improved AI summarizer using BART for real vulnerability summaries.
"""

import os
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch


MODEL_NAME = "facebook/bart-large-cnn"   # Much better summarizer


def load_report(report_path: Path):
    if not report_path.exists():
        raise FileNotFoundError(f"Report not found: {report_path}")
    try:
        return json.loads(report_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError:
        return report_path.read_text(encoding="utf-8")


def chunk_text(text: str, max_chars=3500):
    """Splits long reports into summarizable chunks."""
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]


def summarize_chunk(model, tokenizer, text_chunk):
    """Summarize a chunk safely."""
    inputs = tokenizer.encode(
        text_chunk,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    )

    summary_ids = model.generate(
        inputs,
        max_length=300,
        min_length=50,
        num_beams=4,
        length_penalty=1.5,
        early_stopping=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def generate_final_summary(chunks):
    """Combine chunk summaries into a final polished output."""
    combined = "\n".join(chunks)

    return (
        "### ğŸ” AI Security Summary\n\n"
        "Here is a concise summary of the detected vulnerabilities:\n\n"
        f"{combined}\n\n"
        "---\n"
        "âœ” Summary generated by AI Vulnerability Scanner"
    )


def main():
    report_path = Path("reports/final_report.json")
    output_path = Path("reports/summary.txt")

    print(f"ğŸ“„ Loading merged report: {report_path}")
    report_data = load_report(report_path)

    # Convert to text for summarizer
    if isinstance(report_data, dict):
        text = json.dumps(report_data, indent=2)
    else:
        text = str(report_data)

    print("ğŸ” Splitting report into chunks...")
    chunks = chunk_text(text)

    print(f"ğŸ“¦ Loading summarization model: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

    partial_summaries = []
    for idx, chunk in enumerate(chunks):
        print(f"ğŸ§  Summarizing chunk {idx+1}/{len(chunks)}...")
        partial = summarize_chunk(model, tokenizer, chunk)
        partial_summaries.append(f"- {partial}")

    final_summary = generate_final_summary(partial_summaries)

    print("ğŸ“ Writing final summary...")
    output_path.write_text(final_summary, encoding="utf-8")

    print("âœ… Summary generation complete!")


if __name__ == "__main__":
    main()
